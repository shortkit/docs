---
title: "A/B testing"
description: "Run experiments with config overlay variants to optimize your video feed."
keywords: ["A/B testing", "experiments", "optimization", "variants", "config overlay"]
---

The Experiments section lets you run controlled tests to optimize your feed. Each experiment defines two or more config overlay variants and splits traffic between them, so you can measure the impact of configuration changes on engagement before rolling them out to all users.

## How experiments work

1. **Define variants**: Each variant is a config overlay (or partial overlay) delivered to its assigned user segment
2. **Assign users**: Deterministic assignment based on user ID hash ensures consistent experience per user
3. **Collect metrics**: The platform tracks engagement events per variant automatically
4. **Analyze results**: View statistical significance testing in the results dashboard
5. **Promote winner**: Apply the winning overlay to all users

## Creating an experiment

### Step 1: Basic info

Go to **Experiments > Create New**:

1. **Name**: Descriptive name (e.g., "Scrubber On vs. Off")
2. **Description**: Hypothesis and goals
3. **Type**: What you are testing

### Experiment types

| Type | What you can test |
|------|-------------------|
| **Layout** | Feed height (fullscreen vs. partial), orientation |
| **Controls** | Which controls are visible, auto-hide settings |
| **Theme** | Colors, fonts, control styling |
| **Captions** | Default caption state, caption styling |
| **Ad frequency** | Ad placement intervals, frequency modes |
| **Custom** | Any config overlay parameter |

### Step 2: Define variants

Add 2-4 variants. Each variant is a config overlay fragment:

<Tabs>
  <Tab title="Controls example">
    **Variant A (Control)**: Scrubber enabled
    ```json
    {
      "controls": {
        "scrubber": true
      }
    }
    ```

    **Variant B (Test)**: Scrubber disabled
    ```json
    {
      "controls": {
        "scrubber": false
      }
    }
    ```
  </Tab>
  <Tab title="Layout example">
    **Variant A**: Fullscreen feed
    ```json
    {
      "feedHeight": "fullscreen"
    }
    ```

    **Variant B**: Partial feed with navigation
    ```json
    {
      "feedHeight": { "percentage": 0.85 }
    }
    ```
  </Tab>
  <Tab title="Ad frequency example">
    **Variant A**: Every 5 videos
    ```json
    {
      "adFrequency": {
        "mode": "count",
        "interval": 5
      }
    }
    ```

    **Variant B**: Platform optimized
    ```json
    {
      "adFrequency": {
        "mode": "platform-optimized"
      }
    }
    ```
  </Tab>
</Tabs>

### Step 3: Traffic allocation

Set the percentage of users for each variant:

- **Equal split**: Automatically divide traffic evenly between variants
- **Custom split**: Set specific percentages (must sum to 100%)

<Tip>
  For high-traffic apps, consider starting with a small test population (e.g., 10%) before scaling to full traffic.
</Tip>

### Step 4: Success metrics

Select primary and secondary metrics:

**Primary metrics** (choose 1-2):
- Watch time per session
- Session duration
- Videos watched per session
- Completion rate
- Return rate (next day)

**Secondary metrics** (optional):
- Share rate
- Ad impressions
- Ad revenue
- Rebuffer rate

### Step 5: Launch

Review and launch:
1. Preview how each variant's overlay will look
2. Set experiment duration (recommended: 2 or more weeks)
3. Click **Launch**

## Managing experiments

### Experiment states

| State | Description |
|-------|-------------|
| **Draft** | Not yet launched, can be edited freely |
| **Running** | Actively collecting data |
| **Paused** | Temporarily stopped, can be resumed |
| **Completed** | Reached end date or manually stopped |
| **Archived** | Historical record |

### Monitoring

While an experiment is running, view:
- Users per variant
- Real-time metrics per variant
- Statistical significance progress

### Pausing

Pause an experiment to:
- Investigate unexpected results
- Fix an issue in one variant
- Temporarily stop the traffic split

Paused experiments can be resumed. Users retain their variant assignment.

## Analyzing results

### Results dashboard

View at **Experiments > [Experiment] > Results**:

| Metric | Control | Test | Lift | Confidence |
|--------|---------|------|------|------------|
| Watch time | 45.2s | 52.1s | +15.3% | 94% |
| Completion rate | 42% | 48% | +14.3% | 97% |
| Session length | 4.2min | 4.8min | +14.3% | 91% |

### Statistical significance

Results show:
- **Lift**: Percentage change vs. control
- **Confidence**: Probability that the difference is real (not random)
- **Status**: Significant (95%+), trending, or inconclusive

<Warning>
  Wait for at least 95% confidence before making decisions. Early results often shift as more data is collected.
</Warning>

### Segmented analysis

Break down results by:
- Platform (iOS)
- Region
- User tenure (new vs. returning)
- Device type

## Promoting a winner

When you have statistically significant results:

1. Go to experiment results
2. Click **Promote Variant** on the winning variant
3. Choose:
   - **Apply to 100%**: Immediately apply the winning overlay to all users
   - **Gradual rollout**: Slowly increase from the test percentage

The winning variant's overlay values become part of your organization's production config overlay.

## Best practices

<AccordionGroup>
  <Accordion title="Test one thing at a time">
    Changing multiple overlay values across variants makes it difficult to attribute results. If you want to test both controls and theme, run separate experiments.
  </Accordion>
  <Accordion title="Run long enough">
    Short experiments may produce false positives. Run for at least 2 weeks to capture different usage patterns across weekdays and weekends.
  </Accordion>
  <Accordion title="Consider sample size">
    More users means faster statistical significance. For smaller apps, plan for a longer experiment duration.
  </Accordion>
  <Accordion title="Document hypotheses">
    Write down what you expect to happen and why. This helps interpret results and plan follow-up tests.
  </Accordion>
</AccordionGroup>

### Common experiments

| Goal | What to test |
|------|--------------|
| Increase watch time | Controls visibility, feed layout |
| Improve retention | Session start experience, caption defaults |
| Boost completion rate | Scrubber on/off, content length filters |
| Increase ad revenue | Ad frequency, placement timing |
| Reduce rebuffering | Prefetch depth settings |

## Experiment history

View all past experiments at **Experiments > Completed**:

- Search by name, type, or date
- Filter by outcome (positive lift, no change, negative)
- Export results for reporting

## Next steps

<Columns cols={2}>
  <Card title="Analytics" icon="chart-bar" href="/portal/analytics">
    Deep dive into engagement metrics.
  </Card>
  <Card title="Feed configuration" icon="sliders" href="/portal/feed-configuration">
    Apply experiment learnings to your feed overlay.
  </Card>
</Columns>
